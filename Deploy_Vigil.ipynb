{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabric-Vigil Deployment\n",
    "\n",
    "Deploys the Vigil mirror monitoring solution into the current workspace. Run this notebook\n",
    "once per workspace that contains mirrored databases you want to monitor.\n",
    "\n",
    "## What Gets Deployed\n",
    "\n",
    "| Item | Name | Purpose |\n",
    "|------|------|---------|\n",
    "| Notebook | `nb_vigil_mirror_monitoring` | Checks all mirrored database table health |\n",
    "| Pipeline | `pl_vigil_mirror_monitoring` | Runs the notebook on a schedule |\n",
    "| Schedule | (Cron) | Triggers the pipeline at a configurable interval |\n",
    "| Reflex | `da_vigil_mirror_monitoring` | Sends email alert when the pipeline fails |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### 1. Workspace Permissions\n",
    "\n",
    "The user running this notebook must have **Contributor** or **Admin** role on the target workspace.\n",
    "\n",
    "### 2. Fabric Capacity\n",
    "\n",
    "The target workspace must be on a Fabric capacity (F-SKU or Trial).\n",
    "Data Activator (Reflex) items require Fabric capacity.\n",
    "\n",
    "### 3. (Optional) Service Principal Setup\n",
    "\n",
    "These steps are only required if you switch `AUTH_MODE` to `\"spn\"` in the\n",
    "configuration cell below.\n",
    "\n",
    "**Entra ID App Registration:**\n",
    "1. Navigate to **Entra ID > App registrations > New registration**\n",
    "2. Name it (e.g., `spn-fabric-vigil`) and register\n",
    "3. Under **Certificates & secrets**, create a new client secret and save the value\n",
    "4. Note the **Application (client) ID** and **Directory (tenant) ID**\n",
    "\n",
    "**Fabric Tenant Settings** (requires a Fabric Administrator):\n",
    "\n",
    "| Setting | Location |\n",
    "|---------|----------|\n",
    "| Service principals can use Fabric APIs | Developer settings |\n",
    "| Allow service principals to create and use profiles | Developer settings |\n",
    "\n",
    "For each setting, add a security group that contains your SPN.\n",
    "\n",
    "**Workspace Access:** The SPN must have **Contributor** (or higher) role on the target workspace.\n",
    "\n",
    "**Azure Key Vault (Recommended for SPN):**\n",
    "Store the SPN credentials as Key Vault secrets:\n",
    "- `fabric-spn-client-id`: The Application (client) ID\n",
    "- `fabric-spn-client-secret`: The client secret value\n",
    "\n",
    "The notebook runtime identity must have **Key Vault Secrets User** role or a\n",
    "Key Vault access policy granting **Get** on secrets.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timezone"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set deployment parameters below. At minimum, configure `KEY_VAULT_NAME` (recommended)\n",
    "or the hardcoded credentials for SPN authentication.\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# AUTHENTICATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Authentication mode: \"user\" (default) or \"spn\"\n",
    "#   \"user\" - Items are owned by your personal identity (simplest setup)\n",
    "#   \"spn\"  - Items are owned by a service principal (requires Entra app registration)\n",
    "AUTH_MODE = \"user\"\n",
    "\n",
    "# ==============================================================================\n",
    "# KEY VAULT CONFIGURATION (recommended)\n",
    "# ==============================================================================\n",
    "# Set KEY_VAULT_NAME to retrieve SPN credentials from Azure Key Vault.\n",
    "# Expected secrets: \"fabric-spn-client-id\", \"fabric-spn-client-secret\"\n",
    "KEY_VAULT_NAME = \"\"  # e.g., \"kv-fabric-prod\"\n",
    "\n",
    "# ==============================================================================\n",
    "# HARDCODED CREDENTIALS (development/testing ONLY)\n",
    "# WARNING: Never commit secrets to source control.\n",
    "# These are only used if Key Vault is not configured.\n",
    "# ==============================================================================\n",
    "SP_CLIENT_ID = \"\"\n",
    "SP_CLIENT_SECRET = \"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SCHEDULE\n",
    "# ==============================================================================\n",
    "\n",
    "# How often (in minutes) the monitoring pipeline runs\n",
    "SCHEDULE_INTERVAL_MINUTES = 30\n",
    "\n",
    "# Timezone for the schedule\n",
    "SCHEDULE_TIMEZONE = \"Central Standard Time\"\n",
    "\n",
    "# ==============================================================================\n",
    "# ALERT RECIPIENT\n",
    "# ==============================================================================\n",
    "\n",
    "# Leave empty to auto-detect the executing user's email address.\n",
    "# Set explicitly if alerts should go to someone else (e.g., a team DL).\n",
    "ALERT_EMAIL_OVERRIDE = \"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# GITHUB SOURCE (do not change unless you have forked the repo)\n",
    "# ==============================================================================\n",
    "GITHUB_REPO = \"imtkain/Fabric-Vigil\"\n",
    "GITHUB_BRANCH = \"main\"\n",
    "GITHUB_FILES_PATH = \"files\"\n",
    "\n",
    "# ==============================================================================\n",
    "# ITEM DISPLAY NAMES (customize if desired)\n",
    "# ==============================================================================\n",
    "NOTEBOOK_NAME = \"nb_vigil_mirror_monitoring\"\n",
    "PIPELINE_NAME = \"pl_vigil_mirror_monitoring\"\n",
    "REFLEX_NAME = \"da_vigil_mirror_monitoring\"\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions\n",
    "\n",
    "Logging, JWT decoding, authentication, and API helpers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def log(message: str) -> None:\n",
    "    \"\"\"Print a timestamped status message.\"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{ts}] {message}\")\n",
    "\n",
    "\n",
    "def decode_jwt_claims(token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Decode the payload claims from a JWT access token.\n",
    "\n",
    "    Does not verify the signature (unnecessary for extracting claims\n",
    "    from a token we just acquired from a trusted authority).\n",
    "    \"\"\"\n",
    "    payload_b64 = token.split(\".\")[1]\n",
    "    # Add padding for base64 decoding\n",
    "    payload_b64 += \"=\" * (4 - len(payload_b64) % 4)\n",
    "    return json.loads(base64.b64decode(payload_b64))\n",
    "\n",
    "\n",
    "def get_user_context() -> dict:\n",
    "    \"\"\"\n",
    "    Extract the executing user's UPN and tenant ID from a user-delegated token.\n",
    "\n",
    "    Always uses the interactive user's identity (via notebookutils), regardless\n",
    "    of AUTH_MODE. This captures the human who initiated the deployment for use\n",
    "    as the default alert recipient.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"upn\": str, \"tenant_id\": str}\n",
    "    \"\"\"\n",
    "    log(\"Detecting user context from notebook identity...\")\n",
    "    raw_token = notebookutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n",
    "    token = raw_token.token if hasattr(raw_token, \"token\") else raw_token\n",
    "    claims = decode_jwt_claims(token)\n",
    "\n",
    "    upn = claims.get(\"upn\") or claims.get(\"preferred_username\")\n",
    "    tenant_id = claims.get(\"tid\")\n",
    "\n",
    "    if not upn:\n",
    "        raise ValueError(\n",
    "            \"Could not detect UPN from token claims. \"\n",
    "            \"Set ALERT_EMAIL_OVERRIDE in the configuration cell.\"\n",
    "        )\n",
    "    if not tenant_id:\n",
    "        raise ValueError(\"Could not detect tenant ID (tid) from token claims.\")\n",
    "\n",
    "    log(f\"  UPN:       {upn}\")\n",
    "    log(f\"  Tenant ID: {tenant_id}\")\n",
    "    return {\"upn\": upn, \"tenant_id\": tenant_id}\n",
    "\n",
    "\n",
    "def get_auth_headers(tenant_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Acquire an access token based on AUTH_MODE and return HTTP headers.\n",
    "\n",
    "    SPN mode: Uses MSAL client credentials flow. Credentials are resolved\n",
    "    from Key Vault first, then hardcoded values as fallback.\n",
    "\n",
    "    User mode: Uses notebookutils to get a token for the interactive user.\n",
    "\n",
    "    Args:\n",
    "        tenant_id: Entra tenant ID (detected from user context).\n",
    "\n",
    "    Returns:\n",
    "        dict: Headers with Authorization bearer token and Content-Type.\n",
    "    \"\"\"\n",
    "    if AUTH_MODE == \"spn\":\n",
    "        log(\"Authenticating as service principal...\")\n",
    "\n",
    "        # Resolve credentials: Key Vault takes priority over hardcoded values\n",
    "        client_id = SP_CLIENT_ID\n",
    "        client_secret = SP_CLIENT_SECRET\n",
    "\n",
    "        if KEY_VAULT_NAME:\n",
    "            kv_url = f\"https://{KEY_VAULT_NAME}.vault.azure.net/\"\n",
    "            log(f\"  Retrieving credentials from Key Vault ({KEY_VAULT_NAME})...\")\n",
    "            if not client_id:\n",
    "                client_id = notebookutils.credentials.getSecret(kv_url, \"fabric-spn-client-id\")\n",
    "            if not client_secret:\n",
    "                client_secret = notebookutils.credentials.getSecret(kv_url, \"fabric-spn-client-secret\")\n",
    "\n",
    "        if not client_id or not client_secret:\n",
    "            raise ValueError(\n",
    "                \"SPN authentication requires client ID and client secret. \"\n",
    "                \"Configure KEY_VAULT_NAME or set SP_CLIENT_ID and SP_CLIENT_SECRET.\"\n",
    "            )\n",
    "\n",
    "        from msal import ConfidentialClientApplication\n",
    "        app = ConfidentialClientApplication(\n",
    "            client_id=client_id,\n",
    "            client_credential=client_secret,\n",
    "            authority=f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "        )\n",
    "\n",
    "        result = app.acquire_token_for_client(\n",
    "            scopes=[\"https://api.fabric.microsoft.com/.default\"]\n",
    "        )\n",
    "\n",
    "        if \"access_token\" not in result:\n",
    "            error_desc = result.get(\"error_description\", \"Unknown error\")\n",
    "            raise Exception(f\"SPN token acquisition failed: {error_desc}\")\n",
    "\n",
    "        token = result[\"access_token\"]\n",
    "        log(\"  SPN token acquired\")\n",
    "\n",
    "    elif AUTH_MODE == \"user\":\n",
    "        log(\"Authenticating as current user...\")\n",
    "        raw_token = notebookutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n",
    "        token = raw_token.token if hasattr(raw_token, \"token\") else raw_token\n",
    "        log(\"  User token acquired\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid AUTH_MODE: '{AUTH_MODE}'. Must be 'spn' or 'user'.\")\n",
    "\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "\n",
    "def wait_for_lro(\n",
    "    headers: dict,\n",
    "    response: requests.Response,\n",
    "    item_desc: str,\n",
    "    timeout_seconds: int = 300\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Poll a Fabric long-running operation (LRO) until completion.\n",
    "\n",
    "    Fabric returns 202 Accepted with a Location header for async item creation.\n",
    "    This function polls that URL until the operation succeeds, fails, or times out.\n",
    "\n",
    "    Args:\n",
    "        headers: Auth headers for polling requests.\n",
    "        response: The initial 202 response.\n",
    "        item_desc: Description for log messages (e.g., \"notebook\").\n",
    "        timeout_seconds: Max wait time before raising an error.\n",
    "\n",
    "    Returns:\n",
    "        dict: The final operation result JSON.\n",
    "    \"\"\"\n",
    "    location = response.headers.get(\"Location\")\n",
    "    retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "\n",
    "    if not location:\n",
    "        raise Exception(f\"202 response for {item_desc} but no Location header\")\n",
    "\n",
    "    log(f\"  Waiting for {item_desc} creation (polling every {retry_after}s)...\")\n",
    "\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout_seconds:\n",
    "        time.sleep(retry_after)\n",
    "        poll_resp = requests.get(location, headers=headers)\n",
    "\n",
    "        if poll_resp.status_code == 200:\n",
    "            result = poll_resp.json()\n",
    "            status = result.get(\"status\", \"\").lower()\n",
    "\n",
    "            if status in (\"succeeded\", \"completed\"):\n",
    "                log(f\"  {item_desc} creation completed\")\n",
    "                return result\n",
    "            elif status in (\"failed\", \"cancelled\"):\n",
    "                error = result.get(\"error\", {}).get(\"message\", \"Unknown error\")\n",
    "                raise Exception(f\"{item_desc} creation failed: {error}\")\n",
    "\n",
    "        elapsed = int(time.time() - start)\n",
    "        log(f\"    ... in progress ({elapsed}s elapsed)\")\n",
    "\n",
    "    raise Exception(f\"{item_desc} creation timed out after {timeout_seconds}s\")\n",
    "\n",
    "\n",
    "def find_item_id(\n",
    "    headers: dict,\n",
    "    workspace_id: str,\n",
    "    item_name: str,\n",
    "    item_type: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Look up an item's ID by display name and type in the workspace.\n",
    "\n",
    "    Used after LRO completion when the create response does not include\n",
    "    the item ID directly.\n",
    "\n",
    "    Args:\n",
    "        headers: Auth headers.\n",
    "        workspace_id: Workspace to search.\n",
    "        item_name: Display name to match.\n",
    "        item_type: Fabric item type (e.g., \"Notebook\", \"DataPipeline\", \"Reflex\").\n",
    "\n",
    "    Returns:\n",
    "        str: The item ID.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/items\"\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    for item in resp.json().get(\"value\", []):\n",
    "        if item.get(\"displayName\") == item_name and item.get(\"type\") == item_type:\n",
    "            item_id = item.get(\"id\")\n",
    "            log(f\"  Found {item_type} '{item_name}' (ID: {item_id})\")\n",
    "            return item_id\n",
    "\n",
    "    raise Exception(f\"Could not find {item_type} '{item_name}' in workspace after creation\")\n",
    "\n",
    "\n",
    "def download_github_file(file_path: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Download a file from the Fabric-Vigil GitHub repository.\n",
    "\n",
    "    Uses raw.githubusercontent.com for direct file access.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path relative to repo root (e.g., \"files/pipeline_content.json\").\n",
    "\n",
    "    Returns:\n",
    "        bytes: Raw file content.\n",
    "    \"\"\"\n",
    "    url = f\"https://raw.githubusercontent.com/{GITHUB_REPO}/{GITHUB_BRANCH}/{file_path}\"\n",
    "    log(f\"  Downloading {file_path}...\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.content\n",
    "\n",
    "\n",
    "def create_fabric_item(\n",
    "    headers: dict,\n",
    "    url: str,\n",
    "    body: dict,\n",
    "    item_desc: str,\n",
    "    workspace_id: str,\n",
    "    item_name: str,\n",
    "    item_type: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a Fabric item via REST API, handling both sync and async responses.\n",
    "\n",
    "    Args:\n",
    "        headers: Auth headers.\n",
    "        url: The creation endpoint URL.\n",
    "        body: The JSON request body.\n",
    "        item_desc: Human-readable description for logging.\n",
    "        workspace_id: Workspace ID (for item lookup after LRO).\n",
    "        item_name: Expected display name (for item lookup after LRO).\n",
    "        item_type: Expected Fabric type (for item lookup after LRO).\n",
    "\n",
    "    Returns:\n",
    "        str: The created item's ID.\n",
    "    \"\"\"\n",
    "    resp = requests.post(url, headers=headers, json=body)\n",
    "\n",
    "    if resp.status_code in (200, 201):\n",
    "        item_id = resp.json().get(\"id\")\n",
    "        log(f\"  {item_desc} created (ID: {item_id})\")\n",
    "        return item_id\n",
    "    elif resp.status_code == 202:\n",
    "        wait_for_lro(headers, resp, item_desc)\n",
    "        return find_item_id(headers, workspace_id, item_name, item_type)\n",
    "    else:\n",
    "        # Surface the error details before raising\n",
    "        try:\n",
    "            error_body = resp.json()\n",
    "            log(f\"  Error: {json.dumps(error_body, indent=2)}\")\n",
    "        except Exception:\n",
    "            log(f\"  Error: {resp.status_code} {resp.text}\")\n",
    "        resp.raise_for_status()\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Functions\n",
    "\n",
    "Each function downloads a template from GitHub, replaces placeholders, and creates the item."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def deploy_notebook(headers: dict, workspace_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Download the Vigil monitoring notebook from GitHub and deploy it.\n",
    "\n",
    "    The notebook has no placeholder GUIDs; it discovers everything\n",
    "    dynamically at runtime.\n",
    "\n",
    "    Returns:\n",
    "        str: The created notebook's item ID.\n",
    "    \"\"\"\n",
    "    log(\"=\" * 60)\n",
    "    log(\"DEPLOYING NOTEBOOK\")\n",
    "    log(\"=\" * 60)\n",
    "\n",
    "    nb_content = download_github_file(f\"{GITHUB_FILES_PATH}/nb_vigil_mirror_monitoring.ipynb\")\n",
    "    nb_b64 = base64.b64encode(nb_content).decode(\"utf-8\")\n",
    "\n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/notebooks\"\n",
    "    body = {\n",
    "        \"displayName\": NOTEBOOK_NAME,\n",
    "        \"definition\": {\n",
    "            \"format\": \"ipynb\",\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"path\": \"notebook-content.ipynb\",\n",
    "                    \"payload\": nb_b64,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return create_fabric_item(\n",
    "        headers, url, body, \"Notebook\",\n",
    "        workspace_id, NOTEBOOK_NAME, \"Notebook\"\n",
    "    )\n",
    "\n",
    "\n",
    "def deploy_pipeline(headers: dict, workspace_id: str, notebook_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Download the pipeline template, inject the notebook GUID, and deploy.\n",
    "\n",
    "    Replacements:\n",
    "        __NOTEBOOK_GUID__ -> notebook_id\n",
    "        __WORKSPACE_GUID__ -> workspace_id\n",
    "\n",
    "    Returns:\n",
    "        str: The created pipeline's item ID.\n",
    "    \"\"\"\n",
    "    log(\"=\" * 60)\n",
    "    log(\"DEPLOYING PIPELINE\")\n",
    "    log(\"=\" * 60)\n",
    "\n",
    "    # Download and apply replacements to pipeline content\n",
    "    content_raw = download_github_file(f\"{GITHUB_FILES_PATH}/pipeline_content.json\")\n",
    "    content_str = content_raw.decode(\"utf-8\")\n",
    "    content_str = content_str.replace(\"__NOTEBOOK_GUID__\", notebook_id)\n",
    "    content_str = content_str.replace(\"__WORKSPACE_GUID__\", workspace_id)\n",
    "    content_b64 = base64.b64encode(content_str.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "    # Download platform metadata (no replacements needed)\n",
    "    platform_raw = download_github_file(f\"{GITHUB_FILES_PATH}/pipeline_platform.json\")\n",
    "    platform_b64 = base64.b64encode(platform_raw).decode(\"utf-8\")\n",
    "\n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/dataPipelines\"\n",
    "    body = {\n",
    "        \"displayName\": PIPELINE_NAME,\n",
    "        \"definition\": {\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"path\": \"pipeline-content.json\",\n",
    "                    \"payload\": content_b64,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \".platform\",\n",
    "                    \"payload\": platform_b64,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return create_fabric_item(\n",
    "        headers, url, body, \"Pipeline\",\n",
    "        workspace_id, PIPELINE_NAME, \"DataPipeline\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_pipeline_schedule(headers: dict, workspace_id: str, pipeline_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a Cron schedule on the deployed pipeline.\n",
    "\n",
    "    Uses the Job Scheduler API. The schedule starts immediately and runs\n",
    "    at the interval specified by SCHEDULE_INTERVAL_MINUTES.\n",
    "\n",
    "    Returns:\n",
    "        str: The schedule ID.\n",
    "    \"\"\"\n",
    "    log(\"=\" * 60)\n",
    "    log(f\"CREATING SCHEDULE (every {SCHEDULE_INTERVAL_MINUTES} min)\")\n",
    "    log(\"=\" * 60)\n",
    "\n",
    "    start_dt = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    end_dt = \"2036-12-31T23:59:59\"\n",
    "\n",
    "    url = (\n",
    "        f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}\"\n",
    "        f\"/items/{pipeline_id}/jobs/Pipeline/schedules\"\n",
    "    )\n",
    "    body = {\n",
    "        \"enabled\": True,\n",
    "        \"configuration\": {\n",
    "            \"startDateTime\": start_dt,\n",
    "            \"endDateTime\": end_dt,\n",
    "            \"localTimeZoneId\": SCHEDULE_TIMEZONE,\n",
    "            \"type\": \"Cron\",\n",
    "            \"interval\": SCHEDULE_INTERVAL_MINUTES\n",
    "        }\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, headers=headers, json=body)\n",
    "\n",
    "    if resp.status_code in (200, 201):\n",
    "        schedule_id = resp.json().get(\"id\")\n",
    "        log(f\"  Schedule created (ID: {schedule_id})\")\n",
    "        return schedule_id\n",
    "    else:\n",
    "        try:\n",
    "            error_body = resp.json()\n",
    "            log(f\"  Error: {json.dumps(error_body, indent=2)}\")\n",
    "        except Exception:\n",
    "            log(f\"  Error: {resp.status_code} {resp.text}\")\n",
    "        resp.raise_for_status()\n",
    "\n",
    "\n",
    "def deploy_reflex(\n",
    "    headers: dict,\n",
    "    workspace_id: str,\n",
    "    pipeline_id: str,\n",
    "    tenant_id: str,\n",
    "    alert_email: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Download the reflex template, inject pipeline GUID, tenant, email, and deploy.\n",
    "\n",
    "    Replacements:\n",
    "        __PIPELINE_GUID__ -> pipeline_id\n",
    "        __WORKSPACE_GUID__ -> workspace_id\n",
    "        __TENANT_GUID__    -> tenant_id\n",
    "        __USER_UPN__       -> alert_email\n",
    "\n",
    "    Returns:\n",
    "        str: The created reflex's item ID.\n",
    "    \"\"\"\n",
    "    log(\"=\" * 60)\n",
    "    log(\"DEPLOYING DATA ACTIVATOR (REFLEX)\")\n",
    "    log(\"=\" * 60)\n",
    "\n",
    "    # Download and apply replacements to reflex entities\n",
    "    entities_raw = download_github_file(f\"{GITHUB_FILES_PATH}/reflex_entities.json\")\n",
    "    entities_str = entities_raw.decode(\"utf-8\")\n",
    "    entities_str = entities_str.replace(\"__PIPELINE_GUID__\", pipeline_id)\n",
    "    entities_str = entities_str.replace(\"__WORKSPACE_GUID__\", workspace_id)\n",
    "    entities_str = entities_str.replace(\"__TENANT_GUID__\", tenant_id)\n",
    "    entities_str = entities_str.replace(\"__USER_UPN__\", alert_email)\n",
    "    entities_b64 = base64.b64encode(entities_str.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "    # Download platform metadata (no replacements needed)\n",
    "    platform_raw = download_github_file(f\"{GITHUB_FILES_PATH}/reflex_platform.json\")\n",
    "    platform_b64 = base64.b64encode(platform_raw).decode(\"utf-8\")\n",
    "\n",
    "    url = f\"https://api.fabric.microsoft.com/v1/workspaces/{workspace_id}/reflexes\"\n",
    "    body = {\n",
    "        \"displayName\": REFLEX_NAME,\n",
    "        \"definition\": {\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"path\": \"ReflexEntities.json\",\n",
    "                    \"payload\": entities_b64,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \".platform\",\n",
    "                    \"payload\": platform_b64,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return create_fabric_item(\n",
    "        headers, url, body, \"Reflex\",\n",
    "        workspace_id, REFLEX_NAME, \"Reflex\"\n",
    "    )\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Run the cell below to deploy the full Vigil solution into this workspace."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# MAIN DEPLOYMENT\n",
    "# ==============================================================================\n",
    "deploy_start = time.time()\n",
    "\n",
    "log(\"=\" * 60)\n",
    "log(\"FABRIC-VIGIL DEPLOYMENT\")\n",
    "log(\"=\" * 60)\n",
    "\n",
    "# Step 1: Detect user context (UPN and tenant ID from the interactive user)\n",
    "user_context = get_user_context()\n",
    "tenant_id = user_context[\"tenant_id\"]\n",
    "alert_email = ALERT_EMAIL_OVERRIDE if ALERT_EMAIL_OVERRIDE else user_context[\"upn\"]\n",
    "log(f\"Alert recipient: {alert_email}\")\n",
    "\n",
    "# Step 2: Get workspace context\n",
    "runtime_ctx = notebookutils.runtime.context\n",
    "workspace_id = runtime_ctx.get(\"currentWorkspaceId\") or runtime_ctx.get(\"workspaceId\")\n",
    "if not workspace_id:\n",
    "    raise ValueError(\"Could not determine workspace ID from notebookutils.runtime.context\")\n",
    "log(f\"Target workspace: {workspace_id}\")\n",
    "\n",
    "# Step 3: Authenticate for API calls (SPN or user, per AUTH_MODE)\n",
    "headers = get_auth_headers(tenant_id)\n",
    "\n",
    "# Step 4: Deploy items in dependency order\n",
    "notebook_id = deploy_notebook(headers, workspace_id)\n",
    "pipeline_id = deploy_pipeline(headers, workspace_id, notebook_id)\n",
    "schedule_id = create_pipeline_schedule(headers, workspace_id, pipeline_id)\n",
    "reflex_id = deploy_reflex(headers, workspace_id, pipeline_id, tenant_id, alert_email)\n",
    "\n",
    "# Summary\n",
    "elapsed = time.time() - deploy_start\n",
    "log(\"\")\n",
    "log(\"=\" * 60)\n",
    "log(\"DEPLOYMENT COMPLETE\")\n",
    "log(\"=\" * 60)\n",
    "log(f\"  Notebook:  {NOTEBOOK_NAME} ({notebook_id})\")\n",
    "log(f\"  Pipeline:  {PIPELINE_NAME} ({pipeline_id})\")\n",
    "log(f\"  Schedule:  Every {SCHEDULE_INTERVAL_MINUTES} min, {SCHEDULE_TIMEZONE} ({schedule_id})\")\n",
    "log(f\"  Reflex:    {REFLEX_NAME} ({reflex_id})\")\n",
    "log(f\"  Alerts to: {alert_email}\")\n",
    "log(f\"  Elapsed:   {elapsed:.1f}s\")\n",
    "log(\"=\" * 60)\n",
    ""
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}